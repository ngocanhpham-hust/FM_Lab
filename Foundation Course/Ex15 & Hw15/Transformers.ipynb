{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc191105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchtext\n",
    "import copy\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0992409",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6778c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d7273fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len = 300):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # create a constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, dim)\n",
    "\n",
    "        ########################\n",
    "        ##   YOUR CODE HERE   ##\n",
    "        ########################\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.dim)\n",
    "        # add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:, :seq_len], requires_grad = False).to(device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a909b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask = None, dropout = None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim = -1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb2a610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-headed attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_head = dim // heads\n",
    "        self.h = heads\n",
    "        self.q_linear = nn.Linear(dim, dim)\n",
    "        self.k_linear = nn.Linear(dim, dim)\n",
    "        self.v_linear = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        bs = q.size(0)\n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.dim_head)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.dim_head)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.dim_head)\n",
    "        # transpose to get dimensions bs * h * sl * dim\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        # calculate attention using the function we will define next\n",
    "        scores = attention(q, k, v, self.dim, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.dim)\n",
    "        output = self.out(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b767fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff = 2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36c10adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim = -1, keepdim = True)) \\\n",
    "        / (x.std(dim = -1, keepdim = True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b5c66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an encoder layer with one multi-head attention layer and one \n",
    "# feed-forward layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5bfc94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model).cuda()\n",
    "    \n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
    "        src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87fc9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c90b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output# we don't perform softmax on the output as this will be handled \n",
    "# automatically by our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ed09add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Tokenize\n",
    "\n",
    "class tokenize(object):\n",
    "    \n",
    "    def __init__(self, lang):\n",
    "        self.nlp = spacy.load(lang)\n",
    "            \n",
    "    def tokenizer(self, sentence):\n",
    "        sentence = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "736afa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating batch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def nopeak_mask(size, opt):\n",
    "    np_mask = np.triu(np.ones((1, size, size)), k = 1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0).to(device)\n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg, opt):\n",
    "    \n",
    "    src_mask = (src != opt.src_pad).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg.to(device)\n",
    "        trg_mask = (trg != opt.trg_pad).unsqueeze(-2).to(device)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, opt)\n",
    "        trg_mask = trg_mask & np_mask   \n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask\n",
    "\n",
    "# patch on Torchtext's batching process that makes it more efficient\n",
    "# from http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\n",
    "\n",
    "class MyIterator:\n",
    "    \"\"\"\n",
    "    Thay thế cho torchtext.legacy.data.Iterator cũ.\n",
    "    Tạo batch dữ liệu thủ công, có sắp xếp độ dài chuỗi (sort_key)\n",
    "    và cơ chế chia batch ngẫu nhiên như bản gốc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size, device, train=True, sort_key=lambda x: len(x[0])):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.train = train\n",
    "        self.sort_key = sort_key\n",
    "        self.create_batches()\n",
    "\n",
    "    def create_batches(self):\n",
    "        \"\"\"\n",
    "        Tạo các batch từ dataset, tương tự Iterator cũ của torchtext.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            def pool(data, random_shuffler):\n",
    "                # Chia dataset thành các nhóm lớn trước (pool)\n",
    "                for p in self._batch(data, self.batch_size * 100):\n",
    "                    # Sắp xếp theo độ dài (để giảm padding)\n",
    "                    p_batch = self._batch(sorted(p, key=self.sort_key), self.batch_size)\n",
    "                    # Shuffle ngẫu nhiên thứ tự batch\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.dataset, torch.randperm)\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in self._batch(self.dataset, self.batch_size):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "    def _batch(self, data, batch_size):\n",
    "        # Chia dữ liệu thành các batch con\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            yield data[i:i + batch_size]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "\n",
    "\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb54357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import os\n",
    "import dill as pickle\n",
    "\n",
    "def read_data(opt):\n",
    "    if opt.src_data is not None:\n",
    "        try:\n",
    "            opt.src_data = open(opt.src_data).read().strip().split('\\n')\n",
    "        except:\n",
    "            print(\"error: '\" + opt.src_data + \"' file not found\")\n",
    "            quit()\n",
    "    \n",
    "    if opt.trg_data is not None:\n",
    "        try:\n",
    "            opt.trg_data = open(opt.trg_data).read().strip().split('\\n')\n",
    "        except:\n",
    "            print(\"error: '\" + opt.trg_data + \"' file not found\")\n",
    "            quit()\n",
    "\n",
    "def create_fields(opt):\n",
    "    spacy_langs = ['en', 'fr', 'de', 'es', 'pt', 'it', 'nl']\n",
    "    src_lang = opt.src_lang[0:2]\n",
    "    trg_lang = opt.trg_lang[0:2]\n",
    "    if src_lang not in spacy_langs:\n",
    "        print('invalid src language: ' + opt.src_lang + 'supported languages : ' + spacy_langs)  \n",
    "    if trg_lang not in spacy_langs:\n",
    "        print('invalid trg language: ' + opt.trg_lang + 'supported languages : ' + spacy_langs)\n",
    "    \n",
    "    print(\"loading spacy tokenizers...\")\n",
    "    \n",
    "    SRC_TOKENIZER = get_tokenizer(\"spacy\", language = opt.src_lang)\n",
    "    TRG_TOKENIZER = get_tokenizer(\"spacy\", language = opt.trg_lang)\n",
    "\n",
    "    SRC = {\n",
    "        \"tokenizer\": SRC_TOKENIZER,\n",
    "        \"vocab\": None\n",
    "    }\n",
    "\n",
    "    TRG = {\n",
    "        \"tokenizer\": TRG_TOKENIZER,\n",
    "        \"vocab\": None\n",
    "    }\n",
    "\n",
    "    return(SRC, TRG)\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"Dataset thay thế cho TabularDataset\"\"\"\n",
    "    def __init__(self, src_data, trg_data, src_tokenizer, trg_tokenizer, max_len):\n",
    "        self.data = []\n",
    "        for s, t in zip(src_data, trg_data):\n",
    "            if len(s.split()) < max_len and len(t.split()) < max_len:\n",
    "                self.data.append((src_tokenizer(s.lower()), trg_tokenizer(t.lower())))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def build_vocab(tokenizer, data, specials=['<pad>', '<sos>', '<eos>', '<unk>']):\n",
    "    \"\"\"Thay thế cho SRC.build_vocab()\"\"\"\n",
    "    def yield_tokens(data_iter):\n",
    "        for text in data:\n",
    "            yield tokenizer(text)\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(data), specials = specials)\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def create_dataset(opt, SRC_TOKENIZER, TRG_TOKENIZER):\n",
    "    \"\"\"Tạo dataset và dataloader thay thế cho TabularDataset + Iterator\"\"\"\n",
    "    print(\"Creating dataset and iterator... \")\n",
    "\n",
    "    df = pd.DataFrame({'src': opt.src_data, 'trg': opt.trg_data})\n",
    "    mask = (df['src'].str.count(' ') < opt.max_strlen) & (df['trg'].str.count(' ') < opt.max_strlen)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    dataset = TranslationDataset(\n",
    "        df['src'].tolist(),\n",
    "        df['trg'].tolist(),\n",
    "        SRC_TOKENIZER,\n",
    "        TRG_TOKENIZER,\n",
    "        opt.max_strlen\n",
    "    )\n",
    "\n",
    "    SRC_VOCAB = build_vocab(SRC_TOKENIZER, df['src'].tolist())\n",
    "    TRG_VOCAB = build_vocab(TRG_TOKENIZER, df['trg'].tolist())\n",
    "\n",
    "    opt.src_pad = SRC_VOCAB['<pad>']\n",
    "    opt.trg_pad = TRG_VOCAB['<pad>']\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        src_batch, trg_batch = zip(*batch)\n",
    "        src_batch = [torch.tensor([SRC_VOCAB[token] for token in s]) for s in src_batch]\n",
    "        trg_batch = [torch.tensor([TRG_VOCAB[token] for token in t]) for t in trg_batch]\n",
    "        src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first = True, padding_valu = SRC_VOCAB['<pad>'])\n",
    "        trg_batch = torch.nn.utils.rnn.pad_sequence(trg_batch, batch_first = True, padding_value = TRG_VOCAB['<pad>'])\n",
    "        return src_batch, trg_batch\n",
    "\n",
    "    train_iter = DataLoader(dataset, batch_size = opt.batchsize, shuffle = True, collate_fn = collate_fn)\n",
    "\n",
    "    opt.train_len = len(train_iter)\n",
    "\n",
    "    return train_iter\n",
    "\n",
    "\n",
    "def get_len(train):\n",
    "\n",
    "    for i, b in enumerate(train):\n",
    "        pass\n",
    "    \n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc7f841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "class CosineWithRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Cosine annealing with restarts.\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : torch.optim.Optimizer\n",
    "    T_max : int\n",
    "        The maximum number of iterations within the first cycle.\n",
    "    eta_min : float, optional (default: 0)\n",
    "        The minimum learning rate.\n",
    "    last_epoch : int, optional (default: -1)\n",
    "        The index of the last epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 T_max: int,\n",
    "                 eta_min: float = 0.,\n",
    "                 last_epoch: int = -1,\n",
    "                 factor: float = 1.) -> None:\n",
    "        # pylint: disable=invalid-name\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.factor = factor\n",
    "        self._last_restart: int = 0\n",
    "        self._cycle_counter: int = 0\n",
    "        self._cycle_factor: float = 1.\n",
    "        self._updated_cycle_len: int = T_max\n",
    "        self._initialized: bool = False\n",
    "        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Get updated learning rate.\"\"\"\n",
    "        # HACK: We need to check if this is the first time get_lr() was called, since\n",
    "        # we want to start with step = 0, but _LRScheduler calls get_lr with\n",
    "        # last_epoch + 1 when initialized.\n",
    "        if not self._initialized:\n",
    "            self._initialized = True\n",
    "            return self.base_lrs\n",
    "\n",
    "        step = self.last_epoch + 1\n",
    "        self._cycle_counter = step - self._last_restart\n",
    "\n",
    "        lrs = [\n",
    "            (\n",
    "                self.eta_min + ((lr - self.eta_min) / 2) *\n",
    "                (\n",
    "                    np.cos(\n",
    "                        np.pi *\n",
    "                        ((self._cycle_counter) % self._updated_cycle_len) /\n",
    "                        self._updated_cycle_len\n",
    "                    ) + 1\n",
    "                )\n",
    "            ) for lr in self.base_lrs\n",
    "        ]\n",
    "\n",
    "        if self._cycle_counter % self._updated_cycle_len == 0:\n",
    "            # Adjust the cycle length.\n",
    "            self._cycle_factor *= self.factor\n",
    "            self._cycle_counter = 0\n",
    "            self._updated_cycle_len = int(self._cycle_factor * self.T_max)\n",
    "            self._last_restart = step\n",
    "\n",
    "        return lrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6231fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-23 22:58:20--  https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/english.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4897403 (4.7M) [text/plain]\n",
      "Saving to: ‘english.txt’\n",
      "\n",
      "english.txt         100%[===================>]   4.67M   399KB/s    in 11s     \n",
      "\n",
      "2025-10-23 22:58:33 (434 KB/s) - ‘english.txt’ saved [4897403/4897403]\n",
      "\n",
      "--2025-10-23 22:58:34--  https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/french.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5938378 (5.7M) [text/plain]\n",
      "Saving to: ‘french.txt’\n",
      "\n",
      "french.txt          100%[===================>]   5.66M  2.30MB/s    in 2.5s    \n",
      "\n",
      "2025-10-23 22:58:37 (2.30 MB/s) - ‘french.txt’ saved [5938378/5938378]\n",
      "\n",
      "Prepended http:// to 'data/french.txt'\n",
      "--2025-10-23 22:58:37--  http://data/french.txt\n",
      "Resolving data (data)... failed: nodename nor servname provided, or not known.\n",
      "wget: unable to resolve host address ‘data’\n",
      "FINISHED --2025-10-23 22:58:37--\n",
      "Total wall clock time: 3.3s\n",
      "Downloaded: 1 files, 5.7M in 2.5s (2.30 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/english.txt\n",
    "!mv english.txt data\n",
    "!wget https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/french.txt data/french.txt\n",
    "!mv french.txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d07acf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(opt, src_vocab, trg_vocab):\n",
    "    \n",
    "    assert opt.d_model % opt.heads == 0\n",
    "    assert opt.dropout < 1\n",
    "\n",
    "    model = Transformer(src_vocab, trg_vocab, opt.d_model, opt.n_layers, opt.heads)\n",
    "       \n",
    "    if opt.load_weights is not None:\n",
    "        print(\"loading pretrained weights...\")\n",
    "        model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights'))\n",
    "    else:\n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p) \n",
    "    \n",
    "    if opt.device == 0:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14c5366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading spacy tokenizers...\n",
      "Creating dataset and iterator... \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# for asking about further training use while true loop, and return\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 84\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m read_data(opt)\n\u001b[1;32m     83\u001b[0m SRC, TRG \u001b[38;5;241m=\u001b[39m create_fields(opt)\n\u001b[0;32m---> 84\u001b[0m opt\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSRC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(opt, \u001b[38;5;28mlen\u001b[39m(SRC\u001b[38;5;241m.\u001b[39mvocab), \u001b[38;5;28mlen\u001b[39m(TRG\u001b[38;5;241m.\u001b[39mvocab))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     87\u001b[0m opt\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mlr, betas \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.98\u001b[39m), eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-9\u001b[39m)\n",
      "Cell \u001b[0;32mIn[47], line 85\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(opt, SRC_TOKENIZER, TRG_TOKENIZER)\u001b[0m\n\u001b[1;32m     82\u001b[0m mask \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m<\u001b[39m opt\u001b[38;5;241m.\u001b[39mmax_strlen) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m<\u001b[39m opt\u001b[38;5;241m.\u001b[39mmax_strlen)\n\u001b[1;32m     83\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[mask]\n\u001b[0;32m---> 85\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTranslationDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msrc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSRC_TOKENIZER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTRG_TOKENIZER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_strlen\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m SRC_VOCAB \u001b[38;5;241m=\u001b[39m build_vocab(SRC_TOKENIZER, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     94\u001b[0m TRG_VOCAB \u001b[38;5;241m=\u001b[39m build_vocab(TRG_TOKENIZER, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[0;32mIn[47], line 58\u001b[0m, in \u001b[0;36mTranslationDataset.__init__\u001b[0;34m(self, src_data, trg_data, src_tokenizer, trg_tokenizer, max_len)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(src_data, trg_data):\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m<\u001b[39m max_len \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m<\u001b[39m max_len:\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mappend((\u001b[43msrc_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, trg_tokenizer(t\u001b[38;5;241m.\u001b[39mlower())))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train_model(model, opt):\n",
    "    \n",
    "    print(\"training model...\")\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    if opt.checkpoint > 0:\n",
    "        cptime = time.time()\n",
    "                 \n",
    "    for epoch in range(opt.epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        print(\"   %dm: epoch %d [%s]  %d%%  loss = %s\" %\\\n",
    "            ((time.time() - start)//60, epoch + 1, \"\".join(' '*20), 0, '...'), end = '\\r')\n",
    "        \n",
    "        if opt.checkpoint > 0:\n",
    "            torch.save(model.state_dict(), 'weights/model_weights')\n",
    "                    \n",
    "        for i, batch in enumerate(opt.train): \n",
    "\n",
    "            src = batch.src.transpose(0,1).to(device)\n",
    "            trg = batch.trg.transpose(0,1).to(device)\n",
    "            trg_input = trg[:, :-1].to(device)\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, opt)\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            opt.optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index = opt.trg_pad)\n",
    "            loss.backward()\n",
    "            opt.optimizer.step()\n",
    "          \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % opt.printevery == 0:\n",
    "                p = int(100 * (i + 1) / opt.train_len)\n",
    "                avg_loss = total_loss/opt.printevery\n",
    "                print(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n",
    "                    ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss))\n",
    "                total_loss = 0\n",
    "            \n",
    "            if opt.checkpoint > 0 and ((time.time()-cptime)//60) // opt.checkpoint >= 1:\n",
    "                torch.save(model.state_dict(), 'weights/model_weights')\n",
    "                cptime = time.time()\n",
    "   \n",
    "   \n",
    "        print(\"%dm: epoch %d [%s%s]  %d%%  loss = %.3f\\nepoch %d complete, loss = %.03f\" %\\\n",
    "        ((time.time() - start) // 60, epoch + 1, \"\".join('#' * (100//5)), \"\".join(' ' * (20-(100//5))), 100, avg_loss, epoch + 1, avg_loss))\n",
    "\n",
    "class Opt(object):\n",
    "    pass\n",
    "        \n",
    "def main():\n",
    "    opt = Opt()\n",
    "    opt.src_data = \"data/english.txt\"\n",
    "    opt.trg_data = \"data/french.txt\"\n",
    "    opt.src_lang = \"en_core_web_sm\"\n",
    "    opt.trg_lang = 'fr_core_news_sm'\n",
    "    opt.epochs = 2\n",
    "    opt.d_model = 512\n",
    "    opt.n_layers = 6\n",
    "    opt.heads = 8\n",
    "    opt.dropout = 0.1\n",
    "    opt.batchsize = 1500\n",
    "    opt.printevery = 100\n",
    "    opt.lr = 0.0001\n",
    "    opt.max_strlen = 80\n",
    "    opt.checkpoint = 0\n",
    "    opt.no_cuda = False\n",
    "    opt.load_weights = None\n",
    "    \n",
    "    # Tự động chọn thiết bị khả dụng\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():  # cho Mac M1/M2\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    opt.device = device\n",
    "    \n",
    "    read_data(opt)\n",
    "    SRC, TRG = create_fields(opt)\n",
    "    opt.train = create_dataset(opt, SRC, TRG)\n",
    "    model = get_model(opt, len(SRC.vocab), len(TRG.vocab)).to(device)\n",
    "\n",
    "    opt.optimizer = torch.optim.Adam(model.parameters(), lr = opt.lr, betas = (0.9, 0.98), eps = 1e-9)\n",
    "\n",
    "    if opt.checkpoint > 0:\n",
    "        print(\"model weights will be saved every %d minutes and at end of epoch to directory weights/\"%(opt.checkpoint))\n",
    "    \n",
    "    train_model(model, opt)\n",
    "\n",
    "\n",
    "    # for asking about further training use while true loop, and return\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e78d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
