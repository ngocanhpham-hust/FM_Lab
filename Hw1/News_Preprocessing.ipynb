{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c09439e4-0b60-4311-b492-c0dfe9caae7c",
   "metadata": {},
   "source": [
    "# News Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc16474-cdc9-41ac-bd82-ba833dbe4296",
   "metadata": {},
   "source": [
    "1. Load Dataset\n",
    "2. Remove Stopwords\n",
    "3. TF-IDF Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d97ae3-144e-4740-b159-ee5c429adbb8",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "* Cho tập gồm $n$ văn bản: $D = \\{d_1, d_2, ... d_n\\}$. Tập từ điển tương ứng được xây dựng từ $n$ văn bản này có độ dài là $m$\n",
    "* Xét văn bản $d$ có $|d|$ từ và $t$ là một từ trong $d$. Mã hóa TF-IDF của $t$ trong văn bản $d$ được biểu diễn:\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\text{tf}_{t, d} &= \\frac{f_t}{|d|} \\\\\n",
    "        \\text{idf}_{t, d} &= \\log\\frac{n}{n_t}, \\ \\ \\ \\ n_t = |\\{d\\in D: t\\in d\\}| \\\\\n",
    "        \\text{tf-idf}_{t d} &= \\text{tf}_{t, d} \\times \\text{idf}_{t, d}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "* Khi đó văn bản $d$ được mã hóa là một vector $m$ chiều. Các từ xuất hiện trong d sẽ được thay bằng giá trị tf-idf tương ứng. Các từ không xuất hiện trong $d$ thì thay là 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ea29db-054e-4a4b-8f06-c6774657857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from pyvi import ViTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea2f0a1-011b-406b-a4b2-5c0fc3416b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = 'news_vnexpress'\n",
    "os.makedirs(\"images\", exist_ok = True) # create a new folder to save image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the hidden file .ipynb_checkpoints\n",
    "import shutil\n",
    "\n",
    "checkpoints_path = os.path.join(_input, '.ipynb_checkpoints')\n",
    "\n",
    "if os.path.exists(checkpoints_path):\n",
    "    shutil.rmtree(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1fb0f01-881c-466a-a57d-8310643cd3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doi-song: 120\n",
      "du-lich: 54\n",
      "phap-luat: 59\n",
      "the-thao: 173\n",
      "thoi-su: 59\n",
      "suc-khoe: 162\n",
      "giai-tri: 201\n",
      "giao-duc: 105\n",
      "kinh-doanh: 262\n",
      "khoa-hoc: 144\n",
      "-----------------------------\n",
      "Total number of samples: 1339\n"
     ]
    }
   ],
   "source": [
    "# Number of samples of each label\n",
    "n = 0\n",
    "for label in os.listdir(_input):\n",
    "    print(f'{label}: {len(os.listdir(os.path.join(_input, label)))}')\n",
    "    n += len(os.listdir(os.path.join(_input, label)))\n",
    "print('-----------------------------')\n",
    "print(f'Total number of samples: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "660f6522-6da1-4643-82f3-dd40587ad423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping:\n",
      "doi-song- 0\n",
      "du-lich- 1\n",
      "giai-tri- 2\n",
      "giao-duc- 3\n",
      "khoa-hoc- 4\n",
      "kinh-doanh- 5\n",
      "phap-luat- 6\n",
      "suc-khoe- 7\n",
      "the-thao- 8\n",
      "thoi-su- 9\n",
      "---------------------------\n",
      "Total number of texts: 1339\n"
     ]
    }
   ],
   "source": [
    "data_train = load_files(container_path = _input, encoding = \"utf-8\")\n",
    "print('mapping:')\n",
    "for i in range(len(data_train.target_names)):\n",
    "        print(f'{data_train.target_names[i]}- {i}')\n",
    "\n",
    "print('---------------------------')\n",
    "# print(data_train.filenames[0:1])\n",
    "# print(data_train.target[0:1])\n",
    "# print(data_train.data[0:1])\n",
    "print(\"Total number of texts: {}\".format(len(data_train.filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9413327a-a0fb-4954-afd3-30ab4949fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Số lượng từ trong từ điển: 12797\n",
      "Kích thước dữ liệu sau khi xử lý: (1339, 12797)\n",
      "Kích thước nhãn tương ứng: (1339,)\n"
     ]
    }
   ],
   "source": [
    "# Load Stopwords data \n",
    "with open(\"vietnamese-stopwords.txt\", encoding = \"utf-8\") as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip().replace(\" \", \"_\") for x in stopwords] \n",
    "\n",
    "# Vectorization\n",
    "count_vector_module = CountVectorizer(stop_words = stopwords) # module process text to count vector\n",
    "rf_preprocess_module = Pipeline([('vect', count_vector_module), ('tfidf', TfidfTransformer()),]) # convert text -> TF vector -> normalized by TF-IDF\n",
    " \n",
    "data_preprocessed = rf_preprocess_module.fit_transform(data_train.data, data_train.target)\n",
    "\n",
    "X = data_preprocessed # features\n",
    "Y = data_train.target # labels\n",
    "\n",
    "print(f\"\\nSố lượng từ trong từ điển: {len(count_vector_module.vocabulary_)}\")\n",
    "print(f\"Kích thước dữ liệu sau khi xử lý: {X.shape}\")\n",
    "print(f\"Kích thước nhãn tương ứng: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96759b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 7, 5, ..., 8, 4, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c587e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.14048828 0.        ]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(X[100].toarray())\n",
    "print(Y[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa642313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(289)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of non-zero features in the 101th sample\n",
    "sum(sum(X[100].toarray() != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d32b9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 289 stored elements and shape (1, 12797)>\n",
      "  Coords\tValues\n",
      "  (0, 81)\t0.015211595534715633\n",
      "  (0, 100)\t0.020769547555053954\n",
      "  (0, 156)\t0.023262873797037953\n",
      "  (0, 188)\t0.04722498744523733\n",
      "  (0, 269)\t0.03324036700472501\n",
      "  (0, 392)\t0.024783841259467605\n",
      "  (0, 397)\t0.034232954192352914\n",
      "  (0, 418)\t0.048918066523847725\n",
      "  (0, 662)\t0.022769929356223215\n",
      "  (0, 909)\t0.033182248640039956\n",
      "  (0, 1194)\t0.05117031195708949\n",
      "  (0, 1209)\t0.10234062391417897\n",
      "  (0, 1219)\t0.05117031195708949\n",
      "  (0, 1271)\t0.016005705134410072\n",
      "  (0, 1590)\t0.034926623903062066\n",
      "  (0, 1631)\t0.023493949966261335\n",
      "  (0, 1783)\t0.04734509560871461\n",
      "  (0, 1866)\t0.05117031195708949\n",
      "  (0, 2076)\t0.014547704347804936\n",
      "  (0, 2101)\t0.02693672471116765\n",
      "  (0, 2111)\t0.02577563465433512\n",
      "  (0, 2135)\t0.04322051581452055\n",
      "  (0, 2140)\t0.01566196368628259\n",
      "  (0, 2159)\t0.01608450478874651\n",
      "  (0, 2170)\t0.02950839772591026\n",
      "  :\t:\n",
      "  (0, 12272)\t0.021259537682082122\n",
      "  (0, 12454)\t0.0758997005019029\n",
      "  (0, 12510)\t0.01778617457985167\n",
      "  (0, 12518)\t0.038832198646960935\n",
      "  (0, 12522)\t0.09355442947181114\n",
      "  (0, 12536)\t0.05488370325838489\n",
      "  (0, 12548)\t0.045752865989427884\n",
      "  (0, 12559)\t0.03206234356763186\n",
      "  (0, 12567)\t0.03324036700472501\n",
      "  (0, 12585)\t0.038767743735542225\n",
      "  (0, 12592)\t0.07519534686809995\n",
      "  (0, 12618)\t0.08000423234784887\n",
      "  (0, 12625)\t0.3318224864003996\n",
      "  (0, 12627)\t0.019288093792759513\n",
      "  (0, 12630)\t0.02417303634575905\n",
      "  (0, 12644)\t0.030193779677554423\n",
      "  (0, 12647)\t0.042689479937610325\n",
      "  (0, 12673)\t0.03173992101554847\n",
      "  (0, 12692)\t0.020769547555053954\n",
      "  (0, 12693)\t0.013885134230282651\n",
      "  (0, 12698)\t0.03935911209707955\n",
      "  (0, 12706)\t0.024927343279465622\n",
      "  (0, 12715)\t0.03437923951819016\n",
      "  (0, 12725)\t0.05122667806048764\n",
      "  (0, 12795)\t0.14048828324700807\n"
     ]
    }
   ],
   "source": [
    "print(X[100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
